#!/usr/bin/env python3
"""
ğŸš€ MASS DATA UPLOADER - LTM SPATIAL MEMORY SYSTEM ğŸš€

UNIVERSAL TEXT DATA PROCESSING:
- Uses LTM EngramManager (spatial valence + hash coords)
- NO LLM bottlenecks - pure algorithmic speed
- Clean coordinate key storage: [x.xxx][y.yyy][z.zzz]...[f.fff]
- Processes ANY folder structure (flat, nested, mixed)
- Supports multiple file formats: .txt, .md, .rst, .text, .csv, .json

FOCUS: Fast, clean, functional mass data ingestion for LTM!
"""

import os
import sys
import json
import csv
import time
from typing import List, Dict, Optional
from EngramManager import EngramManager

def find_data_files(folder_path: str, file_types: Optional[List[str]] = None) -> List[str]:
    """Find all supported data files in folder and subfolders"""
    if file_types is None:
        file_types = ['.txt', '.md', '.rst', '.text', '.csv', '.json']
    
    print(f"ğŸ“ Scanning: {folder_path}")
    print(f"ğŸ” Looking for: {', '.join(file_types)}")
    
    all_files = []
    
    # Recursively find all supported files
    for root, dirs, files in os.walk(folder_path):
        for file in files:
            if any(file.lower().endswith(ext) for ext in file_types):
                full_path = os.path.join(root, file)
                all_files.append(full_path)
    
    print(f"ğŸ“„ Found {len(all_files)} data files")
    return all_files

def read_file_content(file_path: str) -> List[str]:
    """Read file content with format-specific handling"""
    file_ext = os.path.splitext(file_path)[1].lower()
    filename = os.path.basename(file_path)
    
    try:
        if file_ext == '.json':
            return read_json_file(file_path)
        elif file_ext == '.csv':
            return read_csv_file(file_path)
        else:
            return read_text_file(file_path)
    except Exception as e:
        print(f"âŒ Failed to read {filename}: {e}")
        return []

def read_text_file(file_path: str) -> List[str]:
    """Read standard text files"""
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        content = f.read().strip()
    
    if not content:
        return []
    
    return create_text_chunks(content)

def read_json_file(file_path: str) -> List[str]:
    """Read JSON files and extract text content"""
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        data = json.load(f)
    
    chunks = []
    
    def extract_text_from_json(obj, path=""):
        """Recursively extract text from JSON objects"""
        if isinstance(obj, dict):
            for key, value in obj.items():
                new_path = f"{path}.{key}" if path else key
                extract_text_from_json(value, new_path)
        elif isinstance(obj, list):
            for i, item in enumerate(obj):
                new_path = f"{path}[{i}]"
                extract_text_from_json(item, new_path)
        elif isinstance(obj, str) and len(obj.strip()) > 10:
            # Only include meaningful text strings
            chunks.extend(create_text_chunks(obj.strip()))
    
    extract_text_from_json(data)
    return chunks

def read_csv_file(file_path: str) -> List[str]:
    """Read CSV files and extract text content"""
    chunks = []
    
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        # Try to detect delimiter
        sample = f.read(1024)
        f.seek(0)
        
        delimiter = ',' if ',' in sample else '\t' if '\t' in sample else ';'
        
        reader = csv.reader(f, delimiter=delimiter)
        
        for row_idx, row in enumerate(reader):
            for col_idx, cell in enumerate(row):
                if isinstance(cell, str) and len(cell.strip()) > 10:
                    # Add context about CSV location
                    contextual_text = f"Row {row_idx}, Column {col_idx}: {cell.strip()}"
                    chunks.extend(create_text_chunks(contextual_text))
    
    return chunks

def create_text_chunks(content: str, chunk_size: int = 300) -> List[str]:
    """Create manageable text chunks from content"""
    if len(content) <= chunk_size:
        return [content]
    
    chunks = []
    sentences = content.split('.')
    
    current_chunk = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        
        # Add sentence to current chunk
        test_chunk = current_chunk + ". " + sentence if current_chunk else sentence
        
        if len(test_chunk) <= chunk_size:
            current_chunk = test_chunk
        else:
            # Save current chunk and start new one
            if current_chunk:
                chunks.append(current_chunk + ".")
            current_chunk = sentence
    
    # Add final chunk
    if current_chunk:
        chunks.append(current_chunk + ".")
    
    return chunks

def process_mass_data(folder_path: str, 
                     db_path: str = "ltm_mass_data.lmdb",
                     file_types: Optional[List[str]] = None,
                     enable_linking: bool = True,
                     chunk_size: int = 300) -> Dict:
    """
    MAIN MASS DATA PROCESSING FUNCTION - Clean and Fast!
    
    Args:
        folder_path: Path to data folder
        db_path: Database path for storage
        file_types: List of file extensions to process
        enable_linking: Enable semantic linking between memories
        chunk_size: Size of text chunks for processing
        
    Returns:
        Dict: Processing results
    """
    
    print("ğŸš€" * 30)
    print("ğŸš€ MASS DATA UPLOADER - LTM SPATIAL MEMORY ğŸš€")
    print("ğŸš€" * 30)
    print("ğŸ¯ PURE ALGORITHMIC PROCESSING - NO LLM BOTTLENECKS!")
    print("âš¡ TURBO MODE: RAM cache + batch linking for maximum speed!")
    
    start_time = time.time()
    
    # Validate folder
    if not os.path.exists(folder_path):
        print(f"âŒ Folder not found: {folder_path}")
        return {'error': 'Folder not found'}
    
    # Find all data files
    data_files = find_data_files(folder_path, file_types)
    
    if not data_files:
        print("âŒ No supported data files found!")
        return {'error': 'No data files found'}
    
    # Initialize LTM Engram Manager
    print(f"\nğŸ§  Initializing LTM Engram Manager...")
    print(f"ğŸ—„ï¸ Database: {db_path}")
    print(f"ğŸ”— Semantic Linking: {'ENABLED' if enable_linking else 'DISABLED'}")
    print(f"âš¡ SPEED MODE: Minimal logging for bulk processing")
    
    manager = EngramManager(
        db_path=db_path, 
        enable_linking=enable_linking,
        turbo_mode=True,  # Enable TURBO mode for bulk loading
        verbose=False  # SPEED: Disable verbose for bulk processing
    )
    
    # Process files
    print(f"\nğŸ“Š Processing {len(data_files)} files...")
    
    total_chunks = 0
    total_stored = 0
    file_type_stats = {}
    
    for file_idx, file_path in enumerate(data_files, 1):
        filename = os.path.basename(file_path)
        file_ext = os.path.splitext(file_path)[1].lower()
        
        # Track file type statistics
        if file_ext not in file_type_stats:
            file_type_stats[file_ext] = {'files': 0, 'chunks': 0}
        file_type_stats[file_ext]['files'] += 1
        
        # Show progress every 50 files for speed
        if file_idx % 50 == 0 or file_idx == len(data_files):
            elapsed = time.time() - start_time
            rate = total_stored / max(1, elapsed)
            print(f"   ğŸ“Š Files: {file_idx}/{len(data_files)} | Memories: {total_stored:,} | Rate: {rate:.0f}/sec")
        
        # Read file content
        chunks = read_file_content(file_path)
        
        if not chunks:
            continue
        
        total_chunks += len(chunks)
        file_type_stats[file_ext]['chunks'] += len(chunks)
        
        # Store each chunk
        for chunk_idx, chunk in enumerate(chunks):
            # Add comprehensive metadata
            metadata = {
                'source_file': filename,
                'file_path': file_path,
                'file_type': file_ext,
                'chunk_index': chunk_idx,
                'total_chunks_in_file': len(chunks),
                'file_number': file_idx,
                'processing_method': 'ltm_spatial_valence',
                'chunk_size': len(chunk),
                'upload_timestamp': time.time()
            }
            
            # Store in LTM manager
            memory_id = manager.store_memory(chunk, metadata=metadata)
            
            if memory_id is not None:
                total_stored += 1
                
                # SPEED MODE: Show progress every 10,000 memories
                if total_stored % 10000 == 0 and total_stored > 0:
                    elapsed = time.time() - start_time
                    current_rate = total_stored / elapsed
                    print(f"   ğŸš€ MILESTONE: {total_stored:,} memories processed | Rate: {current_rate:.0f}/sec")
                    
                    # Show linking stats summary
                    if enable_linking and hasattr(manager, 'turbo_stats'):
                        turbo_stats = manager.turbo_stats
                        total_links = turbo_stats.get('total_links', 0)
                        avg_links = total_links / max(1, total_stored)
                        print(f"   ğŸ”— Links: {total_links:,} total | Avg: {avg_links:.1f}/memory")
    
    # Final results
    total_time = time.time() - start_time
    
    # Get system stats
    stats = manager.get_system_stats()
    
    results = {
        'files_processed': len(data_files),
        'chunks_created': total_chunks,
        'memories_stored': total_stored,
        'processing_time': total_time,
        'rate': total_stored / total_time if total_time > 0 else 0,
        'database_path': db_path,
        'file_type_stats': file_type_stats,
        'system_stats': stats
    }
    
    print(f"\nğŸ‰ MASS DATA UPLOAD COMPLETE!")
    print(f"ğŸ“„ Files: {results['files_processed']:,}")
    print(f"ğŸ“ Chunks: {results['chunks_created']:,}")
    print(f"ğŸ§  Memories: {results['memories_stored']:,}")
    print(f"âš¡ Time: {results['processing_time']:.1f}s ({results['processing_time']/60:.1f} minutes)")
    print(f"ğŸš€ Speed: {results['rate']:.0f} memories/second")
    print(f"ğŸ—„ï¸ Database: {db_path}")
    print(f"ğŸ“ Coordinate format: [x.xxx][y.yyy][z.zzz]...[f.fff]")
    
    # File type breakdown
    print(f"\nğŸ“Š FILE TYPE BREAKDOWN:")
    for file_type, stats_data in file_type_stats.items():
        print(f"   {file_type}: {stats_data['files']} files â†’ {stats_data['chunks']:,} chunks")
    
    # Performance summary
    if results['memories_stored'] >= 1000:
        print(f"\nğŸš€ TURBO MODE OPTIMIZATIONS:")
        print(f"   â€¢ RAM cache for recent memories (no DB reads)")
        print(f"   â€¢ Batch transactions for maximum speed")
        print(f"   â€¢ Minimal logging (every 10k memories)")
        print(f"   â€¢ Silent linking operations")
        if enable_linking:
            print(f"   â€¢ FULL semantic linking maintained!")
        
        if results['memories_stored'] >= 10000:
            estimated_time_old = results['memories_stored'] * 0.5  # Old rate estimate
            time_saved = estimated_time_old - results['processing_time']
            print(f"   â€¢ Estimated time saved: {time_saved/60:.1f} minutes vs traditional systems")
            print(f"   â€¢ Speed improvement: {(results['rate']/2):.0f}x faster!")
    
    # Switch to SAFE mode after bulk loading
    print(f"\nğŸ›¡ï¸ Switching to SAFE MODE for data durability...")
    manager.switch_to_safe_mode()
    
    # Cleanup
    manager.cleanup()
    
    return results

def main():
    """Main entry point"""
    print("ğŸš€ MASS DATA UPLOADER - LTM SPATIAL MEMORY SYSTEM")
    print("=" * 60)
    
    # Get folder path
    if len(sys.argv) > 1:
        folder_path = sys.argv[1]
    else:
        folder_path = input("ğŸ“ Enter data folder path: ").strip()
    
    if not folder_path:
        print("âŒ No folder path provided!")
        return
    
    # Optional: Get database path
    if len(sys.argv) > 2:
        db_path = sys.argv[2]
    else:
        db_path = input("ğŸ—„ï¸ Enter database path (or press Enter for default): ").strip()
        if not db_path:
            db_path = "ltm_mass_data.lmdb"
    
    # Process mass data
    print(f"\nğŸš€ Starting mass data upload...")
    results = process_mass_data(folder_path, db_path)
    
    if 'error' in results:
        print(f"âŒ Error: {results['error']}")
    else:
        print(f"\nâœ… SUCCESS! Mass data loaded into LTM spatial memory!")
        print(f"ğŸ¯ Ready for fast spatial searches and retrieval!")
        print(f"ğŸ” Use LTM_API.search_similar() to query your data!")

if __name__ == "__main__":
    main()

# ğŸš€ MASS DATA UPLOADER - LTM SPATIAL MEMORY SYSTEM ğŸš€
# 
# Usage: 
#   python mass_data_uploader.py /path/to/data/
#   python mass_data_uploader.py /path/to/data/ custom_database.lmdb
# 
# ğŸ¯ FEATURES:
#   âš¡ Pure algorithmic processing (NO LLM bottlenecks)
#   ğŸ“ Clean coordinate keys: [x.xxx][y.yyy][z.zzz]...[f.fff]
#   ğŸ—„ï¸ Fast LMDB storage with enhanced indexing
#   ğŸ“Š Multi-format support: .txt, .md, .rst, .csv, .json
#   ğŸ”„ Progress tracking and comprehensive statistics
#   ğŸ”— Optional semantic linking between memories
#   ğŸ›¡ï¸ Automatic TURBO â†’ SAFE mode switching
#
# ğŸš€ SPEED: Hundreds of memories per second!
# ğŸ§  STORAGE: LMDB with 9D coordinate-based indexing
# ğŸ“ˆ SCALING: Ready for millions of memories!
# ğŸŒ MISSION: Democratizing AI memory without destroying the planet! 